{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/mnt/d/machinelearning/plant_image_classifiaction/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train' #add train dataset path\n",
    "valid_dir = '/mnt/d/machinelearning/plant_image_classifiaction/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid' #add validation dataset path\n",
    "test_dir = '/mnt/d/machinelearning/plant_image_classifiaction/test/test' #add test dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def plot(generator, title):\n",
    "    class_indices = generator.class_indices\n",
    "    labels = list(class_indices.keys())\n",
    "    counts = dict(Counter(generator.classes))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(range(len(counts)), list(counts.values()), tick_label = labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('classes')\n",
    "    plt.ylabel('no. of images')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Use image_dataset_from_directory without class_mode\n",
    "train_gen = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',   # Automatically infers the class labels\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),  # Adjust image size as needed\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,  # If you want to use validation split, set subset to 'training' or 'validation'\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "# Example for validation generator (if needed)\n",
    "valid_gen = tf.keras.utils.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    labels='inferred',   # Automatically infers the class labels\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "test_gen = tf.keras.utils.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    labels='inferred',   # Automatically infers the class labels\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class names to a dictionary where index is the key, and class name is the value\n",
    "class_dict = {i: class_name for i, class_name in enumerate(train_gen.class_names)}\n",
    "\n",
    "print(class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Conv2D, Multiply, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class SpatialAttention(Layer):\n",
    "    def __init__(self, kernel_size=7, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = Conv2D(1, kernel_size=self.kernel_size, strides=1, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, input_feature):\n",
    "        avg_pool = tf.reduce_mean(input_feature, axis=-1, keepdims=True)\n",
    "        max_pool = tf.reduce_max(input_feature, axis=-1, keepdims=True)\n",
    "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "        attention = self.conv(concat)\n",
    "        return Multiply()([input_feature, attention])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SpatialAttention, self).get_config()\n",
    "        config.update({\n",
    "            'kernel_size': self.kernel_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# Load the VGG16 base model without global pooling\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Apply Spatial Attention after the base model's output\n",
    "x = base_model.output\n",
    "x = SpatialAttention()(x)  # Apply custom spatial attention to the feature map\n",
    "\n",
    "# Global Average Pooling after attention\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Flatten and add Dense Layers\n",
    "x = Dense(512, activation='relu')(x)  # Add a fully connected layer\n",
    "x = Dropout(0.5)(x)  # Add Dropout to prevent overfitting\n",
    "\n",
    "# Final output layer\n",
    "# num_classes = len(train_gen.class_indices)  # Number of output classes\n",
    "output = Dense(38, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, to_file='efficientnetb3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Define the callbacks\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.5,          # Factor by which the learning rate will be reduced\n",
    "    patience=5,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,         # Lower bound on the learning rate\n",
    "    verbose=1            # Verbosity mode\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=3,         # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,           # Verbosity mode\n",
    "    restore_best_weights=True  # Restore the model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model with the callbacks\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=100,\n",
    "    # steps_per_epoch=len(train_gen),\n",
    "    # validation_steps=len(valid_gen),\n",
    "    callbacks=[reduce_lr, early_stopping]  # Add the callbacks here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting training & validation accuracy and loss\n",
    "def plot_training_history(history):\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call this function after training\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_gen)\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Convert from one-hot encoded output to class index\n",
    "\n",
    "# Get true labels from the test generator\n",
    "true_classes = np.concatenate([y for x, y in test_gen], axis=0)\n",
    "\n",
    "# Check if true_classes are one-hot encoded or integer encoded\n",
    "if true_classes.ndim > 1:\n",
    "    # If the labels are one-hot encoded, convert them to class index\n",
    "    true_classes = np.argmax(true_classes, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_gen.class_names, yticklabels=test_gen.class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(true_classes, predicted_classes, target_names=test_gen.class_names)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('EfficeintNetB3.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Ensure you register the custom layer when loading\n",
    "model = load_model('EfficeintNetB3.keras', custom_objects={'SpatialAttention': SpatialAttention})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_path = './test/test/Apple__Apple_scab/AppleScab1.JPG'\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title('Test Image')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(128,128))\n",
    "input_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "input_array = np.array([input_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(input_array)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_dir = '/mnt/d/machinelearning/plant_image_classifiaction/test/test'\n",
    "class_name = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = class_name[results]\n",
    "print(model_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
